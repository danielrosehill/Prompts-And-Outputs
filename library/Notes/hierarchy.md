---
title: "A hierarchy for organising LLM outputs"
---

![alt text](/images/androids.png)

While working on various relational database-backed systems to manage the storage of the *things* that LLMs can generate (text, code samples, data visualisations, comparison matrices, and much more) I had to do some thinking, along the way, about "*taxonomy*". 

While I started thinking along these lines for purely technical reasons (ie, defining a schema, which necessitates thinking about questions like *"what tables does it make sense to have in this database and how are they related?*), that process has matured into a wider approach that informs the way I organise all my work with LLMs, which I use for both professional and personal applications. 

At least at the time of writing (Nov 24), this is how I think about things purely from the standpoint of data relationships and hierarchies:

# Models, Fine-Tuned Models, Custom Assistants

Discrete models are the foundational elements of the large language model (LLM) world. 

Given the enormous computational resources that their development necessitates, there remain *relatively* few of these in existence. 

## "Group 1": Fine-Tunes, Custom Assistants

*The magic that makes LLMs work*

As subsets of "models," there are two derivations that look similar (or rather, seem to be aimed to achieve similar ends) but which are really fundamentally quite different. 

Fine-tuned models involve making fundamental changes to the model's underlying mechanisms and training it on new training data to refine its ability to perform a specific task. Among other use-cases, fine-tuning allows generalist models to become more targeted in their application.

A much shallower degree of customisation - but one that's on the contrary, accessible to just about anyone - is simply adding some instructions to a model that persist between chats. 

ChatGPT brought this feature to market as "custom GPTs" and in the process raised huge awareness about how valueable even small modifications to LLMs' configuration parameters can be. Hugging Face allows users to create similar "spins".

From the narrow viewpoint of data hierarchies, it wouldn't be accurate to say that custom assistants are derivatives of fine-tunes. While that's possible, much more commonly they're adjustments to "mainstream" models. 

So while it's somewhat misleading to depict them as parallel processes one level beneath models, it's probably the most accurate reflection that charting can offer. 

![alt text](/images/models.png)

Below is a summary of the main differences:

| Aspect                   | Fine-Tuned Models                             | Custom Assistants (Custom GPTs)                |
|--------------------------|-----------------------------------------------|------------------------------------------------|
| **Core Modification**     | Changes internal weights of the model         | No changes to core; uses instructions/documents|
| **Customization Depth**   | Deep customization for specific tasks/domains | Shallow customization through instructions     |
| **Cost & Complexity**     | Expensive, resource-intensive                 | Cheaper, easier to set up                      |
| **Use Case Suitability**  | Best for highly specialized tasks             | Best for general-purpose assistants            |
| **Adaptability**          | Harder to update frequently                   | Easy to update by modifying instructions       |

---

## "Group" 2: Prompts, Inputs, Instructions

*The things we send in to LLMs to derive useful information*

Returning to the idea of viewing how the various components of LLM work intersect from the somewhat unnatural lens of hierarchial data relationships, we'll exclude "instructions" from the category of inputs (in this context, "instructions" are the customisations that persist over individual chats; in other words, custom assistant configurations). They're a user input, but they fit more tidily under the previous rubric. 

Prompts are inputs to the LLM chain. But that doesn't mean that they're necessarily written by humans. Prompts can be generated in batches programatically or created in chains, with one prompt creating one output and a second prompt being autogenerated based on the output. 

Sticking to the bland but wide verbiage of `inputs` and `outputs` (to describe instructions and inferences) is more linguistically freeing. But these could be represented in a hierarchy as follows:

```
- inputs
--human-prompts
--machine-prompts
```

If you wanted to distinguish one from the other. 

## "Group" 3: Outputs

Finally, we have the things that come "out" of the LLMs. 

I like to call these `outputs` because - although a bit deadpan and flat - it's sufficiently ambiguous.

`Responses` is also fairly accurate. Like `inputs` it's important to remember that there are distinct "worlds" of LLM interactions and that the best terminology (in my opinion) is wide enough to encompass both (the *worlds* I refer to are casual consumer access via web UIs and enterprise and programmatic access through API usage).

An `output` might be a single response to a prompt delivered programatically. Or it might be a more casual response as part of an ongoing "dialogue" with the user.

## Conversations As A "Wrapper" Entity

In the typical use-case of somebody "chatting" with an LLM, there are a *series* of inputs (prompts) and outputs (responses) that together are part of a wider "thing". 

Typically this is called a `chat` or a `conversation`. 

Conversations might include some elements that don't really fall into `inputs` or `outputs` as we've defined them above. An example is an automated initial system message: it didn't come from the user so it's not a prompt; and it's not really a response to any query, so it's not really an `output` either. Therefore, when capturing LLM outputs, not everything contained in a conversation is a natural target for recording and some degree of exclusionary filtering is wise.

So the hierarchy for a conversation looks a bit like this (encompassing the parts I would say are worth keeping and distinguishing between):

```
Chat/
├── prompt_1.txt
├── response_1.txt
├── prompt_2.txt
├── response_2.txt
├── prompt_3.txt
├── response_3.txt
└── chat_history.txt  # Full conversation log (optional)
```
## Prompt And Output Groups (Concatenated Elements)

Using concatenation, it's easy to create "spinoffs" that it may be more useful to record (ie, rather than look at one prompt, let's capture the user's prompts during this chat as a segmented but continuous text file):

```
Chat/
├── concatenated_prompts.txt   # All user prompts combined
├── concatenated_outputs.txt   # All LLM responses combined
└── chat_history.txt           # Full conversation log (optional)
```

---

## Putting It All Together

I'll get around to diagramming this soon, but in words, here's a rough approximation of how (I see) the elements relating to one another.

We have a **user**:

-- The user might be a human  
-- The user might be (another) computer  

The **user** uses something called a large language model (LLM).

But the **LLM** might actually be:

-- Just a (vanilla) LLM accessed through a chat UI  
-- A fine tuned model  
-- A custom assistant  
-- Any of the above but with a custom-built RAG pipeline

The user has as **conversation** with the LLM. A conversation is approximately a cohesive (simulated) conversation about a discrete topic and with the user hoping to achieve a specific goal from interacting with the LLM/

The **conversation** has its own constituent elements:

-- Prompts that the user (or computer) sends for completion  
-- Outputs from the model
