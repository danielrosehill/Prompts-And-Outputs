---
title: "Generate a context doc for my current system for storing LLM outputs"
---
![GPT-4o](https://img.shields.io/badge/GPT--4o-3333FF?style=for-the-badge&logo=openai&logoColor=white)



*Note: this repository consists of the outputs of large language models (LLMs). In many cases, these are unedited or minimally edited. The information provided is for demonstrative purposes only and is intended to highlight constraints and capabilities in LLM applications and prompt engineering strategies.*


# Prompt

Could you create a context document just describing the current way I'm using ChatGPT?
I'm accessing ChatGPT via the web UI
Because I wish to save the outputs and prompts and context I generate for later use, I have an Obsidian vault on my desktop.
In the Obsidian vault I have folders for:
- agent configs
- prompts
- outputs
- context snippets

When I have a useful interaction with an LLM and want to save the output, I will save a new output using a template that has space for prompt and output. Later I'll use a Python script to automatically copy out the prompts in bulk.

I do this so that I can retain and build up a growing collection of prompts and outputs for later use and reference. I use an Obsidian plugin to remotely sync my vault to B2 cloud storage so that I remain in control of it. 

# Output

Currently, I use **ChatGPT** through the **web UI** to interact with the model. My primary aim is to save the prompts, outputs, and context I generate during these interactions for **future reference** and reuse. To achieve this, I manage a dedicated **Obsidian vault** on my desktop that helps keep all this information organized and accessible.

Within my Obsidian vault, I have established four key folders for managing different types of content:
- **Agent Configs**: Stores configuration details for specific LLM agents I use.
- **Prompts**: Collects the prompts I create or find useful for prompting ChatGPT.
- **Outputs**: Holds the responses or content generated by the LLM based on these prompts.
- **Context Snippets**: Contains various pieces of contextual information I use to enhance or customize prompts.

When I have a useful interaction with an LLM, I save it in the **Outputs** folder using a template that includes dedicated spaces for both the **prompt** and the **output**. I then use a **Python script** to **extract prompts in bulk**, allowing me to build a comprehensive library of useful inputs and outputs that I can refer back to at a later time.

To keep control of all my data, I use an **Obsidian plugin** to **remotely sync** my vault to **B2 cloud storage**. This approach ensures that I have reliable backups of my prompt library and all context generated from my interactions with ChatGPT, giving me complete ownership of my data.

